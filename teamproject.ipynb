{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302abfa-dbd6-48ca-bd02-6ba8d2811add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_log_error, median_absolute_error, explained_variance_score\n",
    "\n",
    "#load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "oil_data = pd.read_csv('oil.csv')\n",
    "holiday_data = pd.read_csv('holidays_events.csv')\n",
    "\n",
    "#imputers for handling missing values\n",
    "numerical_imputer = SimpleImputer(strategy='mean')  # For numerical columns\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')  # For categorical columns\n",
    "\n",
    "#filter out rows where 'transferred' is True\n",
    "holiday_data = holiday_data[holiday_data['transferred'] == False]\n",
    "holiday_dates = set(holiday_data['date'])\n",
    "\n",
    "#merge the oil data with train and test data\n",
    "train_data = pd.merge(train_data, oil_data, on='date', how='left')\n",
    "test_data = pd.merge(test_data, oil_data, on='date', how='left')\n",
    "\n",
    "#add a binary column to train and test data indicating if the date is a holiday\n",
    "train_data['is_holiday'] = train_data['date'].apply(lambda x: 1 if x in holiday_dates else 0)\n",
    "test_data['is_holiday'] = test_data['date'].apply(lambda x: 1 if x in holiday_dates else 0)\n",
    "\n",
    "#fill missing oil prices using forward fill\n",
    "train_data['dcoilwtico'].ffill(inplace=True)\n",
    "test_data['dcoilwtico'].ffill(inplace=True)\n",
    "\n",
    "#preprocessing pipelines\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', numerical_imputer),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', categorical_imputer),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, ['onpromotion', 'dcoilwtico', 'is_holiday', 'store_nbr']),\n",
    "        ('cat', categorical_pipeline, ['family'])\n",
    "    ])\n",
    "\n",
    "#hyperparas\n",
    "param_distributions = {\n",
    "    'model__n_estimators': randint(50, 1000), #made this large since data set is in the millions of rows\n",
    "    'model__learning_rate': uniform(0.01, 0.3), \n",
    "    'model__max_depth': randint(2, 10),  \n",
    "    'model__min_samples_split': randint(2, 10),  \n",
    "    'model__min_samples_leaf': randint(1, 10), \n",
    "    'model__max_features': ['sqrt', 'log2'],  \n",
    "    'model__subsample': uniform(0.5, 0.5),  #fraction of samples to be used for fitting the individual base learners\n",
    "}\n",
    "\n",
    "#scale the target variable 'sales' to range 0-1\n",
    "target_scaler = MinMaxScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(train_data[['sales']])\n",
    "\n",
    "#splitting the data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data.drop('sales', axis=1), \n",
    "    y_train_scaled, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#flattening y_train and y_val for model fitting and validation\n",
    "y_train = y_train.ravel()\n",
    "y_val = y_val.ravel()\n",
    "\n",
    "\n",
    "#pipeline with GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "#randomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=10, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fitting\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "#validation\n",
    "val_predictions_scaled = best_model.predict(X_val)\n",
    "\n",
    "#inverse transform the scaled predictions\n",
    "val_predictions = target_scaler.inverse_transform(val_predictions_scaled.reshape(-1, 1))\n",
    "\n",
    "y_val_original = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val_original, val_predictions))\n",
    "mae = mean_absolute_error(y_val_original, val_predictions)\n",
    "r2 = r2_score(y_val_original, val_predictions)\n",
    "msle = mean_squared_log_error(y_val_original, val_predictions)\n",
    "median_ae = median_absolute_error(y_val_original, val_predictions)\n",
    "explained_variance = explained_variance_score(y_val_original, val_predictions)\n",
    "\n",
    "print(f'Validation RMSE: {rmse}')\n",
    "print(f'Validation MAE: {mae}')\n",
    "print(f'Validation R-squared: {r2}')\n",
    "print(f'Validation MSLE: {msle}')\n",
    "print(f'Validation Median Absolute Error: {median_ae}')\n",
    "print(f'Validation Explained Variance Score: {explained_variance}')\n",
    "\n",
    "# Final predictions on test data (can be used as needed)\n",
    "# X_test = test_data.drop('id', axis=1)\n",
    "# test_data['sales'] = best_model.predict(X_test)\n",
    "# output = test_data[['id', 'sales']]\n",
    "# output.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff4b59-c1f7-444f-941d-f00b7e20a8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine_learning]",
   "language": "python",
   "name": "conda-env-.conda-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
